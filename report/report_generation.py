import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from openai import OpenAI
import re
import numpy as np 
from plumbum.cmd import latexmk
from plumbum import local
import networkx as nx
from postprocess.visualization import Visualization, convert_to_edges
from postprocess.judge_functions import edges_to_relationship
from report.help_functions import *
import ast
import json 

def compile_tex_to_pdf_with_refs(tex_file, output_dir=None, clean=True):
    """
    Silently compile a TeX file to PDF with multiple passes for references
    
    Args:
        tex_file (str): Path to the .tex file
        output_dir (str, optional): Output directory for the PDF
        clean (bool): Whether to clean auxiliary files after compilation
    
    Returns:
        bool: True if compilation successful, False otherwise
    """
    try:
        tex_dir = os.path.dirname(tex_file)
        if output_dir is None:
            output_dir = tex_dir

        # Multiple passes for references 
        try:
            # Build latexmk arguments
            args = [
                '-pdf',                     # Generate PDF output
                '-interaction=nonstopmode', # Don't stop for errors
                '-halt-on-error',           # Stop on errors
                '-f',
                '-bibtex',                   # Use bibtex for references
                f'-output-directory={output_dir}'
            ]
                
            # Add input file
            args.append(tex_file)
            
            # Run latexmk
            with local.env(TEXINPUTS=":./"):
                latexmk[args]()
        except Exception as e:
            print(f"Error in compilation pass")
            print(str(e))
            return False
        
        print(f"Successfully compiled {tex_file} to {output_dir}")
        return True
            
    except Exception as e:
        print(f"Exception occurred: {str(e)}")
        return False

class Report_generation(object):
    def __init__(self, global_state, args):
        """
        :param global_state: a dict containing global variables and information
        :param args: arguments for the report generation
        """
        ######## Load the chosen global state and record all global states history ########
        if global_state.results.report_selected_index is not None:
            self.global_state_list = []
            for algo in global_state.logging.global_state_logging:
                with open(f'{global_state.user_data.output_graph_dir}/{algo}_global_state.pkl', 'rb') as f:
                    self.global_state_list.append(pickle.load(f))
            global_state = self.global_state_list[global_state.results.report_selected_index]
        else:
            self.global_state_list = [global_state]
            global_state.logging.global_state_logging = [global_state.algorithm.selected_algorithm]

        self.client = OpenAI()
        self.data_mode = args.data_mode
        self.data_file = args.data_file
        self.global_state = global_state
        self.args = args 
        self.statistics_desc = global_state.statistics.description
        self.knowledge_docs = global_state.user_data.knowledge_docs[0]
        # Data info
        self.data = global_state.user_data.processed_data
        self.statistics = global_state.statistics
        # EDA info
        self.eda_result = global_state.results.eda
        # Result graph matrix
        self.raw_graph = global_state.results.raw_result
        self.graph = global_state.results.converted_graph
        self.revised_graph = global_state.results.revised_graph
        self.bootstrap_probability = global_state.results.bootstrap_probability
        self.original_metrics = global_state.results.metrics
        self.revised_metrics = global_state.results.revised_metrics
        # algo&hp selection prompts
        self.algo = global_state.algorithm.selected_algorithm
        self.algo_can = global_state.algorithm.algorithm_candidates
        self.algo_param = global_state.algorithm.algorithm_arguments_json
        self.prompt = global_state.logging.select_conversation[0]['response']
        self.hp_prompt = global_state.logging.argument_conversation[0]['response']
        # Path to find the visualization graph
        self.visual_dir = global_state.user_data.output_graph_dir

    def get_title(self):
        response_title = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": f"You are a helpful assistant, please give me the name of the given dataset {self.data_file}\n"
                                                "For example, if the dataset is Sachs.csv, then return me with 'Sachs'. If the dataset is a directory called Abalone, then return me with 'Abalone'.\n"
                                                "Only give me the string of name, do not include anything else."},
            ]
        )
        dataset = response_title.choices[0].message.content
        dataset = dataset.replace('_', ' ')
        title = f'Causal Discovery Report on {dataset.capitalize()}'
        return title, dataset
    
    def intro_prompt(self):
        prompt = f"""
        I want to conduct a causal discovery on a dataset and write a report. There are some background knowledge about this dataset.
        1. Please write a brief introduction paragraph. I only need the paragraph, don't include any title.
        2. Do not include any Greek Letters, Please change any Greek Letter into Math Mode, for example, you should change γ into $\gamma$
        
        Background about this dataset: {self.knowledge_docs}
        """
    
        print("Start to find Introduction")
        response_dist = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are an expert in the causal discovery field and helpful assistant."},
                {"role": "user", "content": prompt}
            ]
        )
        response_intro = response_dist.choices[0].message.content
        response_intro = response_intro.replace("_", r"\_")
        return response_intro
    
    def background_prompt(self):
        prompt = f"""
        I want to conduct a causal discovery on a dataset and write a report. There are some background knowledge about this dataset.There are three sections:
### 1. Detailed Explanation about the Variables
### 2. Possible Causal Relations among These Variables
### 3. Other Background Domain Knowledge that may be Helpful for Experts
        **Your Tasks**
        1. Summarize contents in <Section 1. Detailed Explanation about the Variables and Section 3. Other Background Domain Knowledge that may be Helpful for Experts> in 1-2 paragraphs.
        2. I only need the text, do not include title
        3. If you want to use bollet points, make sure it's in latex {{itemize}} format. 
        4. If there are contents like **content** which means the bold fonts, change it into latex bold format \\textbf{{content}}
        Background about this dataset: {self.knowledge_docs}
        """
    
        print("Start to find Background")
        response_background = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are an expert in the causal discovery field and helpful assistant."},
                {"role": "user", "content": prompt}
            ]
        )
        section1 = response_background.choices[0].message.content
        section1 = re.sub(r'.*\*\*(.*?)\*\*', r'\\textbf{\1}', section1)       
        section1 = list_conversion(section1)
        section1 = fix_latex_itemize(section1)
        section1 = bold_conversion(section1)

        col_names = '\t'.join(self.data.columns)
        prompt = f"""
I want to conduct a causal discovery on a dataset and write a report. There is some background knowledge about this dataset.
There are three sections:
### 1. Detailed Explanation about the Variables
### 2. Possible Causal Relations among These Variables
### 3. Other Background Domain Knowledge that may be Helpful for Experts
Please extract all relationships in the second section ### 2. Possible Causal Relations among These Variables, and return in a JSON format
**Thinking Steps**
1. Extract all pairwise relationships, for example A causes B because ....; C causes D because ....; Only include relationships between two variables!
2. Check whether these variables are among {col_names}, please delete contents that include any other variables!
3. Save the result in json, the key is the tuple of pairs, the value is the explanation. 
4. Check whether the json result can be parsed directly, if not you should revise it
This is an example:
You have A causes B because explanation1; C causes D because explanation2
The JSON should be
{{
"(A, B)": explanation1,
"(C, D)": explanation2
}}
**You Must**
1. Only pairwise relationships can be included
2. All variables should be among {col_names}, please delete contents that include any other variables!
3. Only return me a JSON can be parsed directly, DO NOT include anything else like ```!
**Backgroud Knowledge**
{self.knowledge_docs}
"""
        response_background = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are an expert in the causal discovery field and helpful assistant."},
                {"role": "user", "content": prompt}
            ]
        )
        result = response_background.choices[0].message.content
        result = result.strip("```json").strip("```")
        result = json.loads(result)
        result_parsed = {}
        for k, v in result.items():
            # Remove parentheses and split by comma
            key = tuple(x.strip() for x in k.strip('()').split(','))
            result_parsed[key] = v
        #print(result_parsed)
        variables = self.data.columns
        if result_parsed != {}:
            section2 = """
            \\begin{itemize}
            """
            # Potential Relationship Visualization
            zero_matrix = np.zeros((len(variables), len(variables)))
            for pair in result_parsed.keys():
                explanation = result_parsed[pair]
                #pair = ast.literal_eval(pair)
                if pair[0].lower() in variables.str.lower() and pair[1].lower() in variables.str.lower():
                    ind1 = variables.str.lower().get_loc(pair[0].lower())
                    ind2 = variables.str.lower().get_loc(pair[1].lower())
                    zero_matrix[ind2, ind1] = 1
                    section2 += f"\item \\textbf{{{pair[0].replace('_', ' ')} $\\rightarrow$ {pair[1].replace('_', ' ')}}}: {explanation} \n"
            section2 += "\end{itemize}"
        else:
            section2 = ""

        my_visual = Visualization(self.global_state)
        g = nx.from_numpy_array(zero_matrix, create_using=nx.DiGraph)
        # Relabel nodes with variable names from data columns
        mapping = {i: self.data.columns[i] for i in range(len(self.data.columns))}
        g = nx.relabel_nodes(g, mapping)
        pos = nx.spring_layout(g)
        _ = my_visual.plot_pdag(zero_matrix, 'potential_relation.pdf', pos=pos, relation=True)
        relation_path = f'{self.visual_dir}/potential_relation.pdf'
            
        if sum(zero_matrix.flatten())!=0:
            relation_prompt = f"""
            {section2}
            \\begin{{figure}}[H]
            \centering
            \includegraphics[width=0.5\linewidth]{{{relation_path}}}
            \caption{{\label{{fig:relation}}Possible Causal Relation Graph Suggested by LLM}}
            \end{{figure}}
            """
        else:
            relation_prompt = f"""
                {section2}
                """
        return section1, relation_prompt

    def data_prop_prompt(self):
        n, m = self.data.shape
        shape = f'({n}, {m})'
        missingness = 'True' if self.statistics.missingness else 'False'
        data_type = self.statistics.data_type
        linearity = 'True' if self.statistics.linearity else 'False'
        gaussian_error = 'True' if self.statistics.gaussian_error else 'False'
        stationary = 'True' if self.statistics.data_type == 'Time-series' else 'False'
        heterogeneous = 'True' if self.statistics.heterogeneous else 'False'

        prop_table = f"""
        \\begin{{tabular}}{{rrrrrrr}}
            \\toprule
            Shape ($n$ x $d$) & Data Type & Missing Value & Linearity & Gaussian Errors & Time-Series & Heterogeneity \\\\
            \midrule
            {shape}   & {data_type} & {missingness} & {linearity} & {gaussian_error} & {stationary} & {heterogeneous} \\\\
            \\bottomrule
        \end{{tabular}}
        """
        return prop_table
    
    def preprocess_plot_prompt(self):
        if os.path.isfile(f'{self.visual_dir}/residuals_plot.jpg'):
            preprocess_plot = f"""
            The following are Residual Plots and Q-Q Plots for seleted pair of vairables.
            \\begin{{figure}}[H]
                \centering
                \\begin{{subfigure}}{{0.45\\textwidth}}
                    \centering
                    \includegraphics[width=\linewidth]{{{self.visual_dir}/residuals_plot.jpg}}
                    \\vfill
                    \caption{{Residual Plot}}
                \end{{subfigure}}
                \\begin{{subfigure}}{{0.45\\textwidth}}
                    \centering
                    \includegraphics[width=\linewidth]{{{self.visual_dir}/qq_plot.jpg}}
                    \\vfill
                    \caption{{Q-Q Plot}}
                \end{{subfigure}}
            \caption{{Plots for Data Properties Checking}}
            \end{{figure}}   
            """
        else:
            preprocess_plot = ""
        return preprocess_plot

    def eda_prompt(self):
        dist_input_num = self.eda_result['dist_analysis_num']
        dist_input_cat = self.eda_result['dist_analysis_cat']
        corr_input = self.eda_result['corr_analysis']
        
        # Description of distribution
        response_dist_doc = ""
        if dist_input_num != {}:
            response_dist_doc += "Numerical Variables \n \\begin{itemize} \n"
            left_skew_list = []
            right_skew_list = []
            symmetric_list = []
            for feature in dist_input_num.keys():
                if dist_input_num[feature]['mean']<dist_input_num[feature]['median']:
                    left_skew_list.append(feature.replace('_', '\_'))
                elif dist_input_num[feature]['mean']>dist_input_num[feature]['median']:
                    right_skew_list.append(feature.replace('_', '\_'))
                else:
                    symmetric_list.append(feature.replace('_', '\_'))
            response_dist_doc += f"\item Slight left skew distributed variables: {', '.join(left_skew_list) if left_skew_list != [] else 'None'} \n"
            response_dist_doc += f"\item Slight right skew distributed variables: {', '.join(right_skew_list) if right_skew_list != [] else 'None'} \n"
            response_dist_doc += f"\item Symmetric distributed variables: {', '.join(symmetric_list) if symmetric_list != [] else 'None'} \n"
            response_dist_doc += "\end{itemize} \n"
        if dist_input_cat != {}:
            response_dist_doc += "Categorical Variables \n"
            response_dist_doc += "\\begin{itemize} \n"
            for feature in dist_input_cat.keys():
                response_dist_doc += f"\item {feature}: {dist_input_cat[feature]} \n"
            response_dist_doc += "\end{itemize} \n"
        #print('response_dist_doc: ', response_dist_doc)
        # Description of Correlation
        response_corr_doc = "\\begin{itemize} \n"
        high_corr_list = [f"{key[0]}".replace('_', '\\_') + " and " + f"{key[1]}".replace('_', '\\_') for key, value in corr_input.items() if abs(value) > 0.8]
        if len(high_corr_list)>10:
            response_corr_doc += f"\item Strong Correlated Variables ($\geq 0.9$): {', '.join(high_corr_list)}"
            response_corr_doc += ", etc. \n"
        else:
            response_corr_doc += f"\item Strong Correlated Variables ($\geq 0.9$): {', '.join(high_corr_list) if high_corr_list != [] else 'None'} \n"
        med_corr_list = [f"{key[0]}".replace('_', '\\_') + " and " + f"{key[1]}".replace('_', '\\_') for key, value in corr_input.items() if (abs(value) <= 0.8 and abs(value) > 0.5)]
        if len(med_corr_list)>10:
            response_corr_doc += f"\item Moderate Correlated Variables ($0.1-0.9$): {', '.join(med_corr_list)}"
            response_corr_doc += ", etc. \n"
        else:
            response_corr_doc += f"\item Moderate Correlated Variables ($0.1-0.9$): {', '.join(med_corr_list) if med_corr_list != [] else 'None'} \n"
        low_corr_list = [f"{key[0]}".replace('_', '\\_') + " and " + f"{key[1]}".replace('_', '\\_') for key, value in corr_input.items() if abs(value) <= 0.5]
        if len(low_corr_list)>10:
            response_corr_doc += f"\item Weak Correlated Variables ($\leq 0.1$): {', '.join(low_corr_list)}"
            response_corr_doc += ", etc. \n"
        else:
            response_corr_doc += f"\item Weak Correlated Variables ($\leq 0.1$): {', '.join(low_corr_list) if low_corr_list != [] else 'None'} \n"
        response_corr_doc += "\end{itemize} \n"
        #print('response_corr_doc: ',response_corr_doc)
        return response_dist_doc, response_corr_doc
             

    def algo_selection_prompt(self):
        algo_candidates = self.algo_can
        response = """
        \\begin{itemize}
        """
 
        for algo in algo_candidates:
            sub_block = f"""
            \item \\textbf{{{algo}}}:
            \\begin{{itemize}}
                \item \\textbf{{Description}}: {algo_candidates[algo]['description']}
                \item \\textbf{{Justification}}: {algo_candidates[algo]['justification']}
            \end{{itemize}}
                         """
            response += sub_block

        response += """
                    \end{itemize}
                    """
        return response
    
    def param_selection_prompt(self):
        params = self.algo_param['hyperparameters']
        response = """
        \\begin{itemize}
        """
 
        for param in params:
            sub_block = f"""
            \item 
            \\textbf{{{params[param]['full_name']}}}:
            \\begin{{itemize}}
                \item \\textbf{{Value}}: {params[param]['value']}
                \item \\textbf{{Explanation}}: {params[param]['explanation']}
            \end{{itemize}}
                         """
            response += sub_block

        response += """
                    \end{itemize}
                    """
        return response

    def procedure_prompt(self):
        algo_list = self.algo_selection_prompt()
        param_list = self.param_selection_prompt()

        response = f"""
        In this section, we provide a detailed description of the causal discovery process implemented by Causal Copilot. 
        We also provide the chosen algorithms and hyperparameters, along with the justifications for these selections.
        \subsection{{Data Preprocessing}}
        In this initial step, we preprocessed the data and examined its statistical characteristics. 
        This involved cleaning the data, handling missing values, and performing exploratory data analysis to understand distributions and relationships between variables.
                
        \subsection{{Algorithm Recommendation assisted with LLM}}
        Following data preprocessing, we employed a large language model (LLM) to assist in 
        selecting appropriate algorithms for causal discovery based on the statistical characteristics of the dataset and relevant background knowledge. 
        The top three chosen algorithms, listed in order of suitability, are as follows:   
        {algo_list}
        Considering data properties, algorithm capability and user's instruction, the final algorithm we choose is [ALGO].
        \subsection{{Hyperparameter Values Proposal assisted with LLM}}
        Once the algorithms were selected, the LLM aided in proposing hyperparameters 
        for the chosen algorithm, which are specified below:
        {param_list}
        """

        if self.args.data_mode == 'real':
            response += f"""
            \subsection{{Graph Tuning with Bootstrap and LLM Suggestion}}
            In the final step, we performed graph tuning with suggestions provided by the Bootstrap and LLM.
            
            Firstly, we use the Bootstrap technique to get how much confidence we have on each edge in the initial graph.
            If the confidence probability of a certain edge is greater than 90\% and it is not in the initial graph, we force it.
            Otherwise, if the confidence probability is smaller than 10\% and it exists in the initial graph, we forbid it.
            For those existing edges and moderate confidence edges, we utilize LLM to double check their existence and direction according to its knowledge repository.
            
            In this step LLM can use background knowledge to add some edges that are neglected by Statistical Methods, delete and redirect some unreasonable relationships.
            Voting techniques are used to enhance the robustness of results given by LLM, and the results given by LLM should not change results given by Bootstrap.
            Finally, we use Kernel-based Independence Test to remove redundant edges added by LLM hallucination.
            By integrating insights from both of Bootsratp and LLM to refine the causal graph, we can achieve improvements in graph's accuracy and robustness.
            """
        return response
    
    def graph_effect_prompts(self):
        """
        Prompts for Initial Graph Analysis integrated with background knowledge
        Provide following infos:
        1. Relationship of the initial graph that has been converted into natural language
        2. Variable names
        3. Don't include Bootstrap infos here
        """
        if self.graph.sum() == 0:
            response_doc =  "⚠️ According to the given dataset, we cannot find any causal relationship among variables. Please provide more samples or check the data quality.\n"
        else:
            variables = self.data.columns
            edges_dict = convert_to_edges(self.algo, variables, self.graph)
            relation_text_dict, relation_text = edges_to_relationship(self.data, edges_dict)

            prompt = f"""
            The following text describes the causal relationship among variables:
            {relation_text}
            You are an expert in the causal discovery field and are familiar with background knowledge of these variables: {variables.tolist()}
            1. Please write one paragraph to describe the causal relationship, list your analysis as bullet points clearly.
            2. If variable names have meanings, please integrate background knowledge of these variables in the causal relationship analysis.
            Please use variable names {variables[0]}, {variables[1]}, ... in your description.
            3. Do not simply list out all relationships! You should summarize them and give some conclusions.
            4. Do not include any Letters, Please change any Greek Letter into Math Mode, for example, you should change γ into $\gamma$
            
            For example:
            The result graph shows the causal relationship among variables clearly. The {variables[1]} causes the {variables[0]}, ...
            """
            print("Start to find graph effect")
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "You are an expert in the causal discovery field and helpful assistant."},
                    {"role": "user", "content": prompt}
                ]
            )
            response_doc = response.choices[0].message.content
            response_doc = response_doc.replace('_', '\_')
        return response_doc

    def graph_revise_prompts(self):
        if self.bootstrap_probability is not None:
            response = f"""
            By using the method mentioned in the Section 4.4, we provide a revise graph pruned with Bootstrap and LLM suggestion.
            Pruning results are as follows.
            """
            if self.global_state.results.bootstrap_errors != []:
                    response += f"""
                    The following are force and forbidden results given by Bootstrap:
                    
                    {', '.join(self.global_state.results.bootstrap_errors)}
                    """
            else:
                response += f"""
                Bootstrap doesn't force or forbid any edges.
                """
            llm_evaluation_json = self.global_state.results.llm_errors
            direct_record = llm_evaluation_json['direct_record']
            forbid_record = llm_evaluation_json['forbid_record']
            
            if  forbid_record != {} and forbid_record is not None:
                response += f"""
                The following relationships are forbidden by LLM:
                
                \\begin{{itemize}}
                """
                for item in forbid_record.values():
                    response += f"""
                    \item \\textbf{{{item[0][0]} $\\rightarrow$ {item[0][1]}}}: {item[1]}
                    """
                response += f"""
                \end{{itemize}}
                """     
            else:
                response += f"""
                LLM doesn't forbid any edges.
                """
                
            llm_direction_reason = direct_record  
            if llm_direction_reason!={} and llm_direction_reason is not None:
                response += f"""
                    The following are directions confirmed by the LLM:
                    \\begin{{itemize}}
                    """
                for item in llm_direction_reason.values():
                    response += f"""
                    \item \\textbf{{{item[0][0]} $\\rightarrow$ {item[0][1]}}}: {item[1]}
                    """
                response += f"""
                \end{{itemize}}
                """ 
            else:
                response += f"""
                LLM doesn't decide any direction of edges.
                """
            response = response.replace('_', '\_')
            response += """
            This structured approach ensures a comprehensive and methodical analysis of the causal relationships within the dataset.
            """

            response += fr"""
            \begin{{figure}}[H]
            \centering
            \includegraphics[height=0.3\textheight]{{{self.visual_dir}/{self.algo}_revised_graph.pdf}}
            \caption{{Revised Graph}}
            \end{{figure}}
            """

        else:
            response = 'You have skipped the Pruning and Reliability Analysis.'
        #print('graph revise prompt: ', response)
        return response
    
    def confidence_graph_prompts(self):
        ### generate graph layout ###
        name_map = {'certain_edges': 'Directed Edge', #(->)
                    'uncertain_edges': 'Undirected Edge', #(-)
                    'bi_edges': 'Bi-Directed Edge', #(<->)
                    'half_certain_edges': 'Non-Ancestor Edge', #(o->)
                    'half_uncertain_edges': 'edge of non-ancestor ($o-$)', #(o-)
                    'none_edges': 'No D-Seperation Edge', #(o-o)
                    'none_existence':'No Edge'}
        graph_text = """
        \\begin{figure}[H]
            \centering
        """
        if self.bootstrap_probability is not None:
            bootstrap_dict = {k: v for k, v in self.bootstrap_probability.items() if v is not None and sum(v.flatten())>0}
            zero_graphs = [k for k, v in self.bootstrap_probability.items() if  v is not None and sum(v.flatten())==0]
            length = round(1/len(bootstrap_dict), 2)-0.01
            for key in bootstrap_dict.keys():
                graph_path = f'{self.visual_dir}/{key}_confidence_heatmap.jpg'
                caption = f'{name_map[key]}'
                graph_text += f"""
                \\begin{{subfigure}}{{{length}\\textwidth}}
                        \centering
                        \includegraphics[width=\linewidth]{{{graph_path}}}
                        \\vfill
                        \caption{{{caption}}}
                    \end{{subfigure}}"""
            
            graph_text += """
            \caption{Confidence Heatmap of Different Edges}
            \end{figure}    
            """
            ### Generate text illustration
            text_map = {'certain_edges': 'directed edge ($->$)', #(->)
                        'uncertain_edges': 'undirected edge ($-$)', #(-)
                        'bi_edges': 'edge with hidden confounders ($<->$)', #(<->)
                        'half_certain_edges': 'edge of non-ancestor ($o->$)', #(o->)
                        'half_uncertain_edges': 'edge of non-ancestor ($o-$)', #(o-)
                        'none_edges': 'egde of no D-Seperation set', #(o-o)
                        'none_existence':'No Edge'}
            graph_text += "The above heatmaps show the confidence probability we have on different kinds of edges, including "
            for k in bootstrap_dict.keys():
                graph_text += f"{text_map[k]}, "
            zero_graphs = [k.replace("_", "-") for k in zero_graphs]
            graph_text += "The heatmap of " + ', '.join(zero_graphs) + " is not shown because probabilities of all edges are 0. "
        else:
            graph_text = 'You have skipped the Pruning and Reliability Analysis.'
        return graph_text
        

    def confidence_analysis_prompts(self):
        if self.bootstrap_probability is not None:
            edges_dict = self.global_state.results.raw_edges
            relation_text_dict, relation_text = edges_to_relationship(self.data, edges_dict, self.bootstrap_probability)
            variables = '\t'.join(self.data.columns)
            prompt = f"""
            The following text describes the causal relationship among variables from a statisical perspective:
            {relation_text}
            We use traditional causal discovery algorithm to find this relationship, and the probability is calculated with bootstrapping.
            This result is solely from statistical perspective, so it is not reliable enough.
            You are an expert in the causal discovery field and are familiar with background knowledge of these variables: {variables}
            Based on this statistical confidence result, and background knowledge about these variables,
            Please write a paragraph to analyze the reliability of this causal relationship graph. 
            
            **Your Task**
            Firstly, briefly describe how we get these probability with 1-2 sentences.
            Secondly, categorize and these relationships into 3 types and list them out: 
            High Confidence Level (probability>=0.9), 
            Moderate Confidence Level (probability between 0.1 and 0.9), 
            Low Confidence Level (probability<=0.1)
            Do Not include Numerical Numbers in your text!
            
            **Template**
            To evaluate how much confidence we have on each edge, we conducted bootstrapping to calculate the probability of existence for each edge.
            From the Statistics perspective, we can categorize the edges' probability of existence into three types:
            \\begin{{itemize}}
            \item \\textbf{{High Confidence Edges}}: ...
            \item \\textbf{{Moderate Confidence Edges}}: ...
            \item \\textbf{{Low Confidence Edges}}: ...
            \end{{itemize}}
            **You Must**
            1. Follow the template above, do not include anything else like  ```
            2. Write in a professional and concise way, and include all relationships provided.
            3. The list must be in latex format
            """
            print("Start to analyze graph reliability")
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "You are an expert in the causal discovery field and helpful assistant."},
                    {"role": "user", "content": prompt}
                ]
            )
            response_doc = response.choices[0].message.content
            response_doc = response_doc.replace('%', '\%')
            response_doc = response_doc.replace('_', '\_')
            #print('reliability analysis:',response_doc)
        else:
            response_doc = ''
        return response_doc

    def refutation_analysis_prompts(self):
        if self.global_state.results.refutation_analysis is not None:
            text = f"""
                    \\begin{{figure}}[H]
                        \centering
                        \includegraphics[width=0.7\\textwidth]{{{self.global_state.user_data.output_graph_dir}/refutation_graph.jpg}}
                        \caption{{Refutation Graph}}
                    \end{{figure}} \n
                    """
            text += self.global_state.results.refutation_analysis.replace('_', '\_')
            text = text.replace('%', '\%')
        else:
            text = 'You have skipped the refutation analysis step.'
        return text 
    
    def comparision_prompt(self):
        if len(self.global_state_list) < 2:
            return "", ""
        else:
            graph_text = """
            \subsection{Result Graph Comparision}
            \\begin{figure}[H]
                \centering
            """
            algos = [state.algorithm.selected_algorithm for state in self.global_state_list]
            length = round(1/len(self.global_state_list), 2)-0.01
            for algo in algos:
                graph_path = f'{self.visual_dir}/{algo}_initial_graph.pdf'
                caption = f'Initial Result Graph of {algo}'
                graph_text += f"""
                \\begin{{subfigure}}{{{length}\\textwidth}}
                        \centering
                        \includegraphics[width=\linewidth]{{{graph_path}}}
                        \\vfill
                        \caption{{{caption}}}
                    \end{{subfigure}}"""            
            graph_text += """
            \caption{Result Graph Comparision of Different Algorithms}
            \end{figure}    
            """

            prompt = f"""
Help me to write a comparison of the following causal discovery results of different algorithms.
**You should cover the following points**:
1. The different edges of different algorithms' causal graphs.
2. The common edges of different algorithms' causal graphs.
3. Which edges are more reliable and why.
**Note**:
1. Only include your comparison text content in the response. Don't include any other things!
2. Do not include any titles or subtitles. I only need the comparison text.
**causal discovery results of different algorithms**:
"""
            for state in self.global_state_list:
                prompt += f"Result of Algorithm {state.algorithm.selected_algorithm}:\n"            
                relation_text_dict, relation_text = edges_to_relationship(self.data, state.results.raw_edges)
                prompt += f"{relation_text}\n"
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system",
                     "content": 
                     f"""
                     You are an expert in the causal discovery field and helpful assistant.                    
                     """},
                    {"role": "user", "content": prompt}
                ]
            )
            response_doc = response.choices[0].message.content
            response_doc = response_doc.replace("_", r"\_")
            response_doc = bold_conversion(response_doc)
            return graph_text, response_doc
        
    def abstract_prompt(self):
        response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system",
                     "content": 
                     f"""
                     You are an expert in the causal discovery field and helpful assistant.                    
                     """},
                    {"role": "user", "content": 
                     f"""
                     Help me to write a short abstract according to the given information. 
                     You should cover what data is analyzed (find it in title and introduction), what methodology we used, what result we got, and what is our contribution.
                     Only include your abstract text content in the response. Don't include any other things like title, etc.
                     Do not include any Greek Letters, Please change any Greek Letter into Math Mode, for example, you should change γ into $\gamma$
                     0. Title: {self.title}
                     1. Introduction: {self.intro_info}
                     2. Selected Algorithm: {self.algo}
                     3. Discovery Procedure: {self.discover_process},
                     4. Graph Result Analysis: {self.graph_prompt},
                     5. Reliability Analysis: {self.reliability_prompt}
                     6. Comparision of different algorithms: {self.result_comparison}
                     """}
                ]
            )

        response_doc = response.choices[0].message.content
        response_doc = response_doc.replace("_", r"\_")
        return response_doc
    
    def keyword_prompt(self):
        response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system",
                     "content": 
                     """
                     You are an expert in the causal discovery field and helpful assistant.                    
                     """},
                    {"role": "user", "content": 
                     f"""
                     Give me some keywords according to the given information, and these keywords are for an academic report.
                    You should seperate each keywords with a comma, like 'keyword1, keyword2, keyword3'.
                     Only include your keywords text in the response. Don't include any other things.
                     Only include 5 most important key words.
                     0. Title: {self.title}
                     1. Abstract: {self.abstract}
                     1. Introduction: {self.intro_info}
                     """}
                ]
            )

        response_doc = response.choices[0].message.content
        return response_doc
    
    def conclusion_prompt(self):
        response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system",
                     "content": 
                     f"""
                     You are an expert in the causal discovery field and helpful assistant.                    
                     """},
                    {"role": "user", "content": 
                     f"""
                     Help me to write a 1-2 paragraphs conclusion according to the given information. 
                     You should cover:
                      1. what data is analyzed (find it in title and introduction)
                      2. what methodology we used (find it in Discovery Procedure)
                      3. what result we got, this point is important (find it in Graph Result Analysis and Graph Revise Procesure)
                      4. what is our contribution, this point is important (summarize it by yourself)
                     Only include your conclusion text content in the response. Don't include any other things like title, etc.
                     0. Title: {self.title}
                     1. Introduction: {self.intro_info}
                     2. Discovery Procedure: {self.discover_process},
                     3. Graph Result Analysis: {self.graph_prompt},
                     4. Graph Revise Procesure: {self.revise_process}
                     5. Reliability Analysis: {self.reliability_prompt}
                     6. Comparision of different algorithms: {self.result_comparison}
                     """}
                ]
            )

        response_doc = response.choices[0].message.content
        response_doc = response_doc.replace("_", r"\_")
        return response_doc
    
    def latex_convert(self, text):
        prompt = f"""
        Please convert this markdown format text into latex format.
        For example, 
        1. for subheadings '## heading', convert it into '\subsection{{heading}}; for subsubheadings '### heading', convert it into '\subsubsection{{heading}}'
        2. for list of items '-item1 -item2 -item3', convert them into 
        '\\begin{{itemize}}
        \item item1
        \item item2
        \item item3
        \end{{itemize}}'
        3. For bolding notation '**text**', convert it into \\textbf{{text}}.
        4. Do not include any Greek Letters, Please change any Greek Letter into Math Mode, for example, you should change γ into $\gamma$
        This is the text you need to convert: {text}
        Only response converted text, please do not include other things like 'Here is the converted text in LaTeX format:'
        """
        response = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are an expert in the causal discovery field and helpful assistant."},
                {"role": "user", "content": prompt}
            ]
        )
        response_doc = response.choices[0].message.content
        return response_doc

    def generation(self, debug=False):
            '''
            generate and save the report
            :param debug: bool, if True, use template directly without LLM
            :return: Str: A technique report explaining all the results for readers
            '''
            if debug:
                # Load appropriate template based on data mode and ground truth
                prompt_template = load_context("report/context/template_debug.tex")
                return prompt_template

            # Non-debug path continues with full report generation
            print("Start to generate the report")
            # Data info
            df = self.data.copy()
            if self.data.shape[1] > 20:  
                # choose 10 columns randomly
                random_columns = np.random.choice(df.columns, size=10, replace=False)
                df = df[random_columns]
            data_preview = df.head().to_latex(index=False)
            if len(self.data.columns) >= 9:
                data_preview = f"""
                \\resizebox{{\\textwidth}}{{!}}{{
                {data_preview}
                }}
                """
            data_prop_table = self.data_prop_prompt()
            # Intro info
            self.title, dataset = self.get_title()
            self.intro_info = self.intro_prompt()
            # Background info
            if self.data_mode == 'real':
                self.background_info1, self.background_info2 = self.background_prompt()
            else:
                self.background_info1, self.background_info2 = '', ''
            # EDA info
            dist_info, corr_info = self.eda_prompt()
            # Procedure info
            self.discover_process = self.procedure_prompt()
            self.preprocess_plot = self.preprocess_plot_prompt()
            # Graph effect info
            self.graph_prompt = bold_conversion(self.global_state.logging.graph_conversion['initial_graph_analysis'])
            self.graph_prompt = list_conversion(self.graph_prompt)
            self.graph_prompt = fix_latex_itemize(self.graph_prompt)
            # Graph Revise info
            if self.data_mode == 'real':
                self.revise_process = self.graph_revise_prompts()
            else:
                self.revise_process = '' 
            # Graph Reliability info
            self.reliability_prompt = self.confidence_analysis_prompts()
            self.confidence_graph_prompt = self.confidence_graph_prompts()
            self.refutation_analysis = self.refutation_analysis_prompts()
            self.result_comparison_graph_text, self.result_comparison = self.comparision_prompt()
            self.abstract = self.abstract_prompt()
            self.conclusion = self.conclusion_prompt()
            

            if self.data_mode == 'simulated':
                if self.global_state.user_data.ground_truth is not None:
                    prompt_template = load_context("report/context/template_simulated.tex")
                else:
                    prompt_template = load_context("report/context/template_simulated_notruth.tex")
            else:
                if self.global_state.user_data.ground_truth is not None:
                    prompt_template = load_context("report/context/template_real.tex")
                else:
                    prompt_template = load_context("report/context/template_real_notruth.tex")

            replacement1 = {
                "[ABSTRACT]": self.abstract.replace("&", r"\&") or "",
                "[INTRO_INFO]": self.intro_info.replace("&", r"\&") or "",
                "[BACKGROUND_INFO1]": self.background_info1.replace("&", r"\&") or "",
                "[BACKGROUND_INFO2]": self.background_info2.replace("&", r"\&") or "",
                "[DATA_PREVIEW]": data_preview or "",
                "[DATA_PROP_TABLE]": data_prop_table or "",
                "[DIST_INFO]": dist_info or "",
                "[CORR_INFO]": corr_info or "",
                "[RESULT_ANALYSIS]": self.graph_prompt.replace("&", r"\&") or "",
                "[DISCOVER_PROCESS]": self.discover_process.replace("&", r"\&") or "",
                "[PREPROCESS_GRAPH]": self.preprocess_plot or "",
                "[REVISE_PROCESS]": self.revise_process.replace("&", r"\&") or "",
                "[RELIABILITY_ANALYSIS]": self.reliability_prompt.replace("&", r"\&") or "",
                "[CONFIDENCE_GRAPH]": self.confidence_graph_prompt or "",
                "[REFUTATION_GRAPH]": self.refutation_analysis or "",
                "[RESULT_COMPARISION]": self.result_comparison.replace("&", r"\&") or "",
                "[CONCLUSION]": self.conclusion.replace("&", r"\&") or ""
            }
            replacement2 = {
                "[TITLE]": self.title or "",
                "[DATASET]": dataset or "",
                "[POTENTIAL_GRAPH]": f'{self.visual_dir}/potential_relation.pdf',
                "[DIST_GRAPH]": self.eda_result['plot_path_dist'] or "",
                "[CORR_GRAPH]": self.eda_result['plot_path_corr'] or "", 
                "[ALGO]": self.algo or "",
                "[RESULT_GRAPH0]": f'{self.visual_dir}/true_graph.pdf',
                "[RESULT_GRAPH1]": f'{self.visual_dir}/{self.algo}_initial_graph.pdf',
                "[RESULT_GRAPH3]": f'{self.visual_dir}/metrics.jpg',
                "[RESULT_GRAPH_COMPARISION]": self.result_comparison_graph_text
            }

            for placeholder, value in replacement1.items():
                prompt_template = prompt_template.replace(placeholder, value)
            for placeholder, value in replacement2.items():
                prompt_template = prompt_template.replace(placeholder, value)
            prompt_template = replace_unicode_with_latex(prompt_template)
            #print(prompt_template)
            return prompt_template
    
    def latex_bug_checking(self, tex_path, num_error_corrections=2):
        save_path = self.global_state.user_data.output_report_dir

        # Iteratively fix any LaTeX bugs
        for i in range(num_error_corrections):
            # Filter trivial bugs in chktex
            check_output = os.popen(f"lacheck {tex_path} -q -n2 -n24 -n13 -n1").read()
            with open(tex_path, 'r', encoding='utf-8') as file:
                tex_content = file.read()
            if check_output:
                response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system",
                     "content": f"""
                     You are a helpful debugging assistant, help me to fix bugs in the latex report I will give you. 
                     1. Please fix the LaTeX errors guided by the output of `chktek`:
                        {check_output}.
                    ** YOU SHOULD **
                     1. Make the minimal fix required and do not change any other contents!
                     2. Only include your latex content in the response which can be rendered to pdf directly. Don't include other things like '''latex '''
                     """},
                    {"role": "user", "content": tex_content}
                ]
                )
                output = response.choices[0].message.content
                with open(f'{save_path}/report_revised.tex', 'w', encoding='utf-8') as file:
                    file.write(output)
            else:
                break

    def save_report(self, report):
        save_path = self.global_state.user_data.output_report_dir
        if not os.path.exists(save_path):
            os.makedirs(save_path)
        with open(f'{save_path}/report.txt', 'w', encoding='utf-8') as file:
            # Write some text to the file
            file.write(report)
        with open(f'{save_path}/report.tex', 'w', encoding='utf-8') as file:
            file.write(report)
        # # fix latex bugs before rendering
        # print('check latex bug')
        # self.latex_bug_checking(f'{save_path}/report.tex')
        # Compile the .tex file to PDF using pdflatex
        print('start compilation')
        compile_tex_to_pdf_with_refs(f'{save_path}/report.tex', save_path)

def test(args, global_state):
    my_report = Report_generation(global_state, args)
    report = my_report.generation()
    my_report.save_report(report)

import argparse
def parse_args():
    parser = argparse.ArgumentParser(description='Causal Learning Tool for Data Analysis')

    # Input data file
    parser.add_argument(
        '--data-file',
        type=str,
        default="demo_data/20250130_183915/2021online_shop/2021online_shop.csv",
        help='Path to the input dataset file (e.g., CSV format or directory location)'
    )

    # Output file for results
    parser.add_argument(
        '--output-report-dir',
        type=str,
        default='demo_data/20250130_183915/2021online_shop/output_report',
        help='Directory to save the output report'
    )

    # Output directory for graphs
    parser.add_argument(
        '--output-graph-dir',
        type=str,
        default='demo_data/20250130_183915/2021online_shop/output_graph',
        help='Directory to save the output graph'
    )

    # OpenAI Settings
    parser.add_argument(
        '--organization',
        type=str,
        default="org-gw7mBMydjDsOnDlTvNQWXqPL",
        help='Organization ID'
    )

    parser.add_argument(
        '--project',
        type=str,
        default="proj_SIDtemBJMHUWG7CPdU7yRjsn",
        help='Project ID'
    )

    parser.add_argument(
        '--apikey',
        type=str,
        default=None,
        help='API Key'
    )

    parser.add_argument(
        '--simulation_mode',
        type=str,
        default="offline",
        help='Simulation mode: online or offline'
    )

    parser.add_argument(
        '--data_mode',
        type=str,
        default="real",
        help='Data mode: real or simulated'
    )

    parser.add_argument(
        '--debug',
        action='store_true',
        default=True,
        help='Enable debugging mode'
    )

    parser.add_argument(
        '--initial_query',
        type=str,
        default="selected algorithm: FGES",
        help='Initial query for the algorithm'
    )

    parser.add_argument(
        '--parallel',
        type=bool,
        default=False,
        help='Parallel computing for bootstrapping.'
    )

    parser.add_argument(
        '--demo_mode',
        type=bool,
        default=False,
        help='Demo mode'
    )

    args = parser.parse_args()
    return args

import pickle  
if __name__ == '__main__':
    args = parse_args()
    with open('demo_data/20250201_151758/heart disease/output_graph/PC_global_state.pkl', 'rb') as file:
        global_state = pickle.load(file)
    test(args, global_state)
    # save_path = 'demo_data/20250130_130622/house_price/output_report'
    # compile_tex_to_pdf_with_refs(f'{save_path}/report.tex', save_path)
    
